---
title: "A brief comparison of speed of Interior Point and EM algorithms"
author: "Matthew Stephens"
date: 2016-01-26
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

First, we load the necessary libraries.

```{r packages}
library(REBayes)
library(ashr)
library(ggplot2)
```

```{r chunk_options, include=FALSE}
# Specify settings for displaying the plots in the rendered document.
source("chunk-options.R")
```

## Set up simulation

The following simulation assumes $\beta \sim N(0,\sigma_{\sf betasd}^2)$ and
with $\hat{\beta}$ having standard deviation 1.

```{r simulation_setup}
timed_sims = function(ash.args,nsim = 20,nmin = 100,nmax = 1e3,betasd = 1) {
  n = 10^seq(log10(nmin),log10(nmax),length = nsim)
  elapsed.time = rep(0,nsim)
  for (i in 1:nsim) {
    set.seed(i)
    with(ash.args,cat(sprintf("%6s %13s %3d\n",method,optmethod,i)))
    betahat = rnorm(n[i],sd = sqrt(1 + betasd^2))
    elapsed.time[i] =
      system.time(do.call(ash,args = modifyList(ash.args,
        list(betahat = betahat, sebetahat = 1))))[3]
  }
  return(data.frame(elapsed.time = elapsed.time,seed = 1:nsim,n = n))
}
```

Now run a simulation study, for $n$ (number of tests; $p$ in paper) in
range 10 to 100,000. Note that the warnings are being generated by the EM
algorithm in big problems due to lack of convergence.

```{r model_fitting, results='hide'}
df = data.frame()
cat("method --optmethod-- sim\n")
for(method in c("fdr","shrink")) {
  for(optmethod in c("mixIP","cxxMixSquarem")) {
    df = rbind(df,data.frame(method = method,optmethod = optmethod,
           timed_sims(list(method = method,optmethod = optmethod),
                           nsim = 50,nmin = 10,nmax = 1e5)))
  }
}
cat("\n")
```

Summarize the model fitting results in a table.

```{r model_fitting_summary}
cat("Average computation time (in seconds):\n")
print(as.table(by(df,df[c("method","optmethod")],
                  function (x) mean(x$elapsed.time))))
```

## Results

Now plot time as a function of $n$:

```{r plot_results}
qplot(x = n,y = elapsed.time,data = df,col = optmethod,facets = .~method,
      ylab = "elapsed time (s)")
```

Zoom-in on "small" problems with $n < 5000$.

```{r plot_results_magnified}
qplot(x = n,y = elapsed.time, data = subset(df,n < 5000),
      col = optmethod,facets = .~method,ylab = "elapsed time (s)")
```

## Summary

The IP method clearly scales to large problem much better than EM. It is faster,
and also more reliable (sometimes reaches higher log-likelihood than EM,
never smaller); see [checkIP.html](checkIP.html).

However, for small problems (n < 5000) the EM is adequate for many practical
purposes, solving within a few seconds. This is particularly true for the
penalty term (method = fdr), which helps the EM converge, presumably because
it is helping identifiability, removing the large flat parts of the likelihood
objective function.

Indeed, for small problems with the penalty term (n < 2000) EM with squarem
acceleration is a little faster in these comparisons than IP.

## Session information

```{r info}
sessionInfo()
```
