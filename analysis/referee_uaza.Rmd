---
title: "Assessment of referee comment regarding q value and zero assumption"
author: "Matthew Stephens"
date: 2016-03-21
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

A referee reported a simulation of 10,000 hypotheses, a fraction 0.8
are null ($\mu_i = 0$), and a fraction 0.2 are alternative with effect
$\mu_i \sim N(0,1)$. For each effect $i$ there are n=30 observations
from $x_{i,j} \sim N(\mu_i,1)$ and a one-sample t test is used to
compute a $p$ value. The referee correctly notes that these
simulations i) obey the unimodal assumption (UA), and ii) have the
property that "most" $z$ scores near 0 are null (Efron's "Zero
Assumption"; ZA). The motivation is to demonstrate that the UA and the
ZA are not contradictory.

I agree with this, if we define the ZA as "most" $z$ scores near 0 are
null.  However, in practice the existing methods effectively make a
stronger assumption (or at least, behave as if they do): that *all*
$z$ scores near 0 are null. Consequently they end up creating a hole
in the distribution of alternative $z$ scores at 0. Here we illustrate
this by applying qvalue to this scenario.

```{r chunk_options, include=FALSE}
# Specify settings for displaying the plots in the rendered document.
source("chunk-options.R")
```

## Simulation

First, we generate a small data set.

```{r sim_data}
set.seed(100)
mu = c(rep(0,8000),rnorm(2000))
x = matrix(rnorm(30*10000),nrow=10000,ncol=30)+mu
ttest.pval = function(x){return(t.test(x)$p.value)}
x.p = apply(x,1,ttest.pval)
```

Here is how qvalue (arguably, implicitly) decomposes the distribution of p values into null and alternative
```{r}
source("../R/nullalthist.R")
lfdr.qv = qvalue::qvalue(x.p)$lfdr
altnullhist(x.p,lfdr.qv,main="p values: qvalue",ncz=40,xlab="p value",cex.axis=0.8)
```


And here is the corresponding plot in z score space. Note the "hole" in the distribution of $z$ scores at 0 under the alternative.
```{r}
ttest.est = function(x){return(t.test(x)$estimate)}
x.est = apply(x,1,ttest.est)
zscore = x.est/(1/sqrt(30))
nullalthist(zscore,lfdr.qv,main="qvalue's implicit partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

For comparison here is the decomposition, using ash, of $z$ scores into null and alternative components.
```{r}
library(ashr)
lfdr.ash=get_lfdr(ash(x.est,1/sqrt(30)))
nullalthist(zscore,lfdr.ash,main="ash partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

For completeness here is (roughly) the ``true" decomposition of $z$
scores into null and alternative components.

```{r}
trueg = ashr::normalmix(c(0.8,0.2),c(0,0),c(0,1))
lfdr.true=get_lfdr(ash(x.est,1/sqrt(30),g=trueg,fixg=TRUE))
nullalthist(zscore,lfdr.true,main="true partition of z \n into null and alternative",ncz=60,cex.axis=0.8,ylim=c(0,0.3),xlim=c(-6,6),xlab="z score")
```

## Explanation

In estimating $\pi_0$ qvalue assumes that *all* p values near 1 are
null. This is equivalent to assuming that *all* z scores near 0 are
null. Thus, despite the fact that qvalue does not explicitly model the
distribution of $p$ values or $z$ scores under the alternative, its
way of estimating $\pi_0$ necessarily creates a hole in the $z$ score
distribution at 0 under the alternative.

## Session information

```{r info}
sessionInfo()
```
